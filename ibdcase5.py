# -*- coding: utf-8 -*-
"""IBDcase5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fi0IUvvXGnD4w0eqPKPQ3HU6GkrM2ewz
"""

# @title 1. Install Java and HBase
# 1. Install Java 8 (HBase 2.5.x works best with Java 8)
!apt-get update -qq > /dev/null
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# 2. Download HBase 2.5.8 (Stable)
!wget -q https://archive.apache.org/dist/hbase/2.5.8/hbase-2.5.8-bin.tar.gz
!tar -xvf hbase-2.5.8-bin.tar.gz > /dev/null

# 3. Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["HBASE_HOME"] = "/content/hbase-2.5.8"
os.environ["PATH"] += ":" + os.environ["HBASE_HOME"] + "/bin"

print("Java and HBase installed successfully.")

# --- Verification Step ---
print("Verifying Java Installation...")
!java -version
if not os.path.exists(os.environ["JAVA_HOME"] + "/bin/java"):
    print(f"Error: Java executable not found at {os.environ['JAVA_HOME']}/bin/java")
else:
    print("Java executable found.")

# @title 2. Configure and Start HBase (Standalone Mode)

# 1. Configure hbase-site.xml for Standalone mode (Local Filesystem)
hbase_site_content = """
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///content/hbase_data</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/content/zookeeper_data</value>
  </property>
  <property>
    <name>hbase.unsafe.stream.capability.enforce</name>
    <value>false</value>
  </property>
</configuration>
"""

with open(f"{os.environ['HBASE_HOME']}/conf/hbase-site.xml", "w") as f:
    f.write(hbase_site_content)

# 2. Patch hbase-env.sh to ensure JAVA_HOME is set correctly
with open(f"{os.environ['HBASE_HOME']}/conf/hbase-env.sh", "a") as f:
    f.write(f'\nexport JAVA_HOME={os.environ["JAVA_HOME"]}\n')

# 3. Start HBase Backend
print("Starting HBase Daemon...")
!{os.environ['HBASE_HOME']}/bin/start-hbase.sh

# 4. Start Thrift Server (Required for Python Client)
print("Starting Thrift Server...")
!{os.environ['HBASE_HOME']}/bin/hbase-daemon.sh start thrift

# 5. Check if processes are running (Should see HMaster and ThriftServer)
!jps

# @title 3. Install HappyBase and Connect
!pip install happybase

import happybase
import collections

# Patch for Python 3.10+ compatibility
try:
    collections.Iterable = collections.abc.Iterable
    collections.Mapping = collections.abc.Mapping
    collections.MutableSet = collections.abc.MutableSet
    collections.MutableMapping = collections.abc.MutableMapping
except AttributeError:
    pass # Python versions < 3.10 don't need this

# Establish connection
# We connect to localhost on default Thrift port 9090
try:
    connection = happybase.Connection('127.0.0.1', 9090)
    connection.open()
    print("Successfully connected to HBase via Thrift!")
except Exception as e:
    print(f"Connection failed: {e}")

# @title Exercise 1: Create 'Employee' Table
table_name = 'employee'
cf_name = 'personal_data'

# 1. Check if table exists, if so, delete it (to start fresh)
if table_name.encode() in connection.tables():
    print(f"Table {table_name} exists. Disabling and deleting...")
    connection.disable_table(table_name)
    connection.delete_table(table_name)

# 2. Create table with one column family 'personal_data'
connection.create_table(
    table_name,
    {cf_name: dict()} # Default configuration for the column family
)

print(f"Table '{table_name}' created successfully.")
print("Current tables:", connection.tables())

# @title Exercise 2: Insert Data
table = connection.table('employee')

# Batch insert for efficiency (or use table.put() for single rows)
# Format: table.put(row_key, { 'family:column': value })

print("Inserting data...")

# Row 1: Alice
table.put(b'1', {
    b'personal_data:name': b'Alice',
    b'personal_data:city': b'New York',
    b'personal_data:role': b'Engineer'
})

# Row 2: Bob
table.put(b'2', {
    b'personal_data:name': b'Bob',
    b'personal_data:city': b'California',
    b'personal_data:role': b'Designer'
})

# Row 3: Charlie (Has an extra column 'salary')
table.put(b'3', {
    b'personal_data:name': b'Charlie',
    b'personal_data:city': b'Boston',
    b'personal_data:salary': b'85000'
})

print("Data inserted successfully.")

# @title Exercise 3: Read and Scan Data

# 1. GET: Fetch a single row by Row Key
row_key = b'1'
row = table.row(row_key)
print(f"--- Fetching Row Key: {row_key.decode()} ---")
for key, value in row.items():
    print(f"{key.decode()}: {value.decode()}")

# 2. SCAN: Iterate over all rows
print("\n--- Scanning All Rows ---")
for key, data in table.scan():
    print(f"Row Key: {key.decode()}")
    for col, val in data.items():
        print(f"  {col.decode()}: {val.decode()}")

# @title Exercise 4: Simple Filter (Python-side)
# Find all employees who live in 'New York'

print("--- Searching for employees in New York ---")

target_city = b'New York'

for key, data in table.scan():
    # Check if the city column exists and matches
    if data.get(b'personal_data:city') == target_city:
        name = data.get(b'personal_data:name').decode()
        role = data.get(b'personal_data:role').decode()
        print(f"Found: {name} (Role: {role})")

import pandas as pd
import sqlite3
import matplotlib.pyplot as plt

# Load your dataset
df = pd.read_csv('/content/netflix_titles_nov_2019.csv')

# Create SQLite DB
conn = sqlite3.connect("netflix.db")
cursor = conn.cursor()

# Create table
cursor.execute("""
CREATE TABLE IF NOT EXISTS netflix (
    title TEXT,
    director TEXT,
    cast TEXT,
    country TEXT,
    date_added TEXT,
    release_year INTEGER,
    rating TEXT,
    duration TEXT,
    listed_in TEXT,
    description TEXT,
    type TEXT,
    show_id TEXT PRIMARY KEY
)
""")

# Insert data into SQL
df.to_sql("netflix", conn, if_exists="replace", index=False)

print("SQL table created successfully!")

query = """
SELECT type, COUNT(*) AS total
FROM netflix
GROUP BY type;
"""
pd.read_sql(query, conn)

query = """
SELECT title, release_year, rating
FROM netflix
WHERE type='Movie' AND release_year > 2015
ORDER BY release_year DESC;
"""
pd.read_sql(query, conn)

query = """
SELECT country, COUNT(*) AS total
FROM netflix
WHERE country IS NOT NULL
GROUP BY country
ORDER BY total DESC
LIMIT 10;
"""
pd.read_sql(query, conn)

query = """
SELECT title, type, date_added
FROM netflix
WHERE date_added LIKE '%2019%';
"""
pd.read_sql(query, conn)

query = """
SELECT director, COUNT(*) AS total
FROM netflix
WHERE director IS NOT NULL
GROUP BY director
ORDER BY total DESC
LIMIT 10;
"""
pd.read_sql(query, conn)

query = """
SELECT title, type, description
FROM netflix
WHERE LOWER(description) LIKE '%love%';
"""
pd.read_sql(query, conn)

query = """
SELECT rating, COUNT(*) as total
FROM netflix
GROUP BY rating
ORDER BY total DESC;
"""
pd.read_sql(query, conn)

result = pd.read_sql("SELECT type, COUNT(*) AS total FROM netflix GROUP BY type", conn)

plt.figure(figsize=(6,4))
plt.bar(result['type'], result['total'])
plt.title("Movies vs TV Shows")
plt.xlabel("Type")
plt.ylabel("Count")
plt.show()

ratings = pd.read_sql("""
SELECT rating, COUNT(*) AS total
FROM netflix
GROUP BY rating
ORDER BY total DESC
""", conn)

# Replace None values in 'rating' column with 'Unknown'
ratings['rating'] = ratings['rating'].fillna('Unknown')

plt.figure(figsize=(10,5))
plt.bar(ratings['rating'], ratings['total'])
plt.xticks(rotation=45)
plt.title("Rating Distribution")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

year_data = pd.read_sql("""
SELECT release_year, COUNT(*) AS total
FROM netflix
GROUP BY release_year
ORDER BY release_year
""", conn)

plt.figure(figsize=(10,5))
plt.plot(year_data['release_year'], year_data['total'])
plt.title("Number of Titles Released per Year")
plt.xlabel("Year")
plt.ylabel("Count")
plt.grid(True)
plt.show()