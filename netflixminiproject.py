# -*- coding: utf-8 -*-
"""Netflixminiproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RykP_yes32nTpvEV3JJH8QAJh3GrYUvI
"""

# ==============================================================================
# CELL 1: Clean Spark Setup and Data Load
# ==============================================================================

# 1. Spark and Java Setup (REQUIRED IN COLAB)
!pip install pyspark findspark -q
# Colab typically has OpenJDK 17 pre-installed, so we'll configure for that.
# No need to install Java 8 unless specifically required.
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null

import os
import findspark
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, avg, count, when, max as max_

# Set JAVA_HOME to the path of the commonly pre-installed OpenJDK 17 in Colab
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

# Get the current Python version for the correct pyspark path
python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
pyspark_path = f"/usr/local/lib/python{python_version}/dist-packages/pyspark"

# Explicitly set SPARK_HOME to the pyspark installation path
# This ensures Spark binaries like spark-submit are found.
os.environ["SPARK_HOME"] = pyspark_path

# Initialize findspark to locate the spark home automatically (will use SPARK_HOME)
findspark.init()

# Stop any existing SparkSession to ensure a clean state
# This block is useful for re-running cells in Colab
if 'spark' in locals() and spark.sparkContext._jsc is not None:
    spark.stop()

spark = SparkSession.builder.appName("Netflix_BigData_Analysis").getOrCreate()
print("Spark Session Initialized.")

# 2. Data Load
file_path = "netflix_titles_nov_2019.csv"
# Load the CSV file into a Spark DataFrame
netflix_df = spark.read.csv(
    file_path,
    header=True,
    inferSchema=True,
    multiLine=True,
    escape='"'
)
netflix_df.cache() # Cache for better performance on repeated operations
netflix_df.createOrReplaceTempView("netflix_titles") # Register temp view for SQL

print("\n--- Initial Data Check ---")
print("Schema:")
netflix_df.printSchema()
print("\nFirst 5 Rows:")
netflix_df.limit(5).show(truncate=False)


# ==============================================================================
# CELL 2: Basic DataFrame Operations (Mirroring your Examples)
# ==============================================================================

print("\n--- Basic Spark Operations ---")

# Analogous to: df.select("Name", "City").show()
print("Titles and Release Years:")
netflix_df.select("title", "release_year").limit(5).show(truncate=False)

# Analogous to: df.filter(df.Salary > 80000).show()
print("Content Released After 2017:")
netflix_df.filter(col("release_year") > 2017).show(5)

# Analogous to: df.groupBy("City").avg("Salary").show()
print("Average Release Year by Content Type:")
netflix_df.groupBy("type").avg("release_year").show()

# Analogous to: df.groupBy("Gender").count().show()
print("Count of Movies vs. TV Shows:")
netflix_df.groupBy("type").count().show()

# Analogous to: df2 = df.withColumn("Salary_Hike_10%", df.Salary * 1.10)
netflix_df_age = netflix_df.withColumn(
    "Age_of_Content_Years",
    (2025 - col("release_year")) # Assuming current year is 2025 for calculation
)
print("Content with Age Column:")
netflix_df_age.select("title", "release_year", "Age_of_Content_Years").show(5, truncate=False)

# Analogous to: df.orderBy(df.Salary.desc()).show()
print("Content Ordered by Newest Release:")
netflix_df.orderBy(col("release_year").desc()).show(5, truncate=False)


# ==============================================================================
# CELL 3: Specific Analysis Tasks (Mirroring your Task Table)
# ==============================================================================

print("\n--- Specific Analysis Tasks ---")

# Task 1: Find the newest release and display the details
# Analogous to: Find the highest salary and display the employee details
newest_release_year = netflix_df.agg(max_("release_year").alias("max_year")).collect()[0]["max_year"]
print(f"Task 1: Highest Release Year is {newest_release_year}")
netflix_df.filter(col("release_year") == newest_release_year).select("title", "release_year", "type").show(5, truncate=False)

# Task 2: Display only content from India and released after 2018
# Analogous to: Display only employees who are from Mumbai and Salary < 75000
print("\nTask 2: Content from 'India' released after 2018:")
netflix_df.filter(
    (col("country") == "India") & (col("release_year") > 2018)
).select("title", "country", "release_year").show(5, truncate=False)

# Task 3: Calculate count of content in each country
# Analogous to: Calculate count of employees in each city
print("\nTask 3: Count of Content in Each Country:")
netflix_df.groupBy("country").agg(count("*").alias("content_count")).orderBy(col("content_count").desc()).show(5, truncate=False)

# Task 4: Add a new column "Category" -> release_year > 2010 = "Modern", else "Classic"
# Analogous to: Add a new column "Category" -> Salary > 90000 = "High", else "Medium"
netflix_df_categorized = netflix_df.withColumn(
    "Content_Era",
    when(col("release_year") > 2010, "Modern")
    .otherwise("Classic")
)
print("\nTask 4: Content with 'Content_Era' Category:")
netflix_df_categorized.select("title", "release_year", "Content_Era").show(5, truncate=False)


# ==============================================================================
# CELL 4: Spark SQL, Export, and Cleanup
# ==============================================================================

print("\n--- Spark SQL Query ---")
# Analogous to: SELECT City, COUNT(*), AVG(Salary) FROM emp GROUP BY City
spark.sql("""
SELECT
    type,
    COUNT(*) AS total_count,
    AVG(release_year) AS average_release_year
FROM netflix_titles
GROUP BY type
""").show()


# Analogous to: df3.write.parquet, spark.read.parquet
# Save final processed DataFrame as Parquet and load again
print("\n--- Export and Import (Parquet) ---")
parquet_output_path = "/content/netflix_processed.parquet"

# Save as Parquet
netflix_df_categorized.write.mode("overwrite").parquet(parquet_output_path)
print(f"Saved to {parquet_output_path}")

# Load Parquet back
parquet_df = spark.read.parquet(parquet_output_path)
print("Parquet DataFrame loaded and showing 5 rows:")
parquet_df.select("title", "Content_Era").show(5, truncate=False)

# Cleanup
spark.stop()
print("\nSpark Session Stopped. Code execution complete.")

# Cell 4: MapReduce Queries (RDD - Detailed Genre Count)
from pyspark.sql.functions import desc, col
from pyspark.sql import SparkSession

# Re-initialize Spark Session if it was stopped
# This ensures an active session for the operations in this cell
if 'spark' not in locals() or spark.sparkContext._jsc is None:
    spark = SparkSession.builder.appName("Netflix_BigData_Analysis").getOrCreate()
    print("Spark Session Re-initialized.")

# Re-load netflix_df as it was created by a previously stopped Spark Session
# Ensure file_path is available or re-declared if needed
file_path = "netflix_titles_nov_2019.csv" # Assuming this file is available
netflix_df = spark.read.csv(
    file_path,
    header=True,
    inferSchema=True,
    multiLine=True,
    escape='"'
)
netflix_df.cache() # Cache for better performance on repeated operations
netflix_df.createOrReplaceTempView("netflix_titles") # Register temp view for SQL

# 1. Get RDD focused on the 'listed_in' column
genre_rdd = netflix_df.select(col("listed_in")).rdd.filter(
    lambda row: row["listed_in"] is not None
).map(lambda row: row["listed_in"].lower())

# 2. MAP step: Split the comma-separated genres and emit (genre, 1) pairs
mapped_rdd = genre_rdd.flatMap(
    lambda genres: [
        (genre.strip(), 1)
        for genre in genres.split(',')
    ]
)

# 3. REDUCE step: Sum the counts for each key (genre)
reduced_rdd = mapped_rdd.reduceByKey(lambda a, b: a + b)

# 4. Final step: Convert to DataFrame, sort and save for Plotly
genre_counts_df = reduced_rdd.toDF(["genre", "count"]).orderBy(desc("count"))

print("Top 10 Most Frequent Genres (MapReduce Result):")
genre_counts_df.show(10)

# Convert to Pandas for Plotly visualization
genre_counts_pd = genre_counts_df.toPandas()

# Cell 5: More Advanced SQL (Window Functions, UDFs)
from pyspark.sql.window import Window
from pyspark.sql.functions import dense_rank, col, udf
from pyspark.sql.types import StringType

# 1. Window Function: Rank titles within each country by release year
# This finds the 'vintage' (oldest) titles within each country.
country_window = Window.partitionBy("country").orderBy(col("release_year").asc())

ranked_titles = spark.sql("""
    SELECT
        title,
        country,
        release_year,
        DENSE_RANK() OVER (PARTITION BY country ORDER BY release_year ASC) as vintage_rank
    FROM netflix_titles
    WHERE country IS NOT NULL AND release_year IS NOT NULL
""")

print("--- 1. Window Function (Ranked Vintage Titles by Country) ---")
ranked_titles.filter(col("country") == "United States").orderBy("vintage_rank").limit(3).show(truncate=False)


# 2. UDF for Text Analysis (Categorizing Description Length)
def categorize_description(text):
    if text is None:
        return "N/A"
    desc_len = len(text)
    if desc_len < 100:
        return "Short"
    elif desc_len < 200:
        return "Medium"
    else:
        return "Long"

# Register the UDF with Spark
spark.udf.register("categorize_description_sql", categorize_description, StringType())

# Use the UDF in a SQL query to analyze description distribution
description_analysis_df = spark.sql("""
    SELECT
        categorize_description_sql(description) AS length_category,
        COUNT(*) AS count
    FROM netflix_titles
    GROUP BY 1
    ORDER BY 2 DESC
""")

print("\n--- 2. UDF & SQL Text Analysis (Description Length Distribution) ---")
description_analysis_df.show(truncate=False)

# Cell 6: Automated Dashboard (Plotly) and Export Options
import plotly.express as px
import pandas as pd

# NOTE: This assumes Cell 4 ran successfully and created 'genre_counts_pd'

print("Generating Plotly Treemap for Genre Distribution...")

# Create a Treemap for interactive exploration of genres
fig_genre_treemap = px.treemap(
    genre_counts_pd.head(20), # Limit to top 20 for cleaner visualization
    path=['genre'],
    values='count',
    title='Top 20 Netflix Genre Distribution',
    color='count',
    color_continuous_scale='Agsunset',
    template='plotly_dark'
)

# Display the interactive Plotly figure
fig_genre_treemap.show()

# Final cleanup
spark.stop()
print("\nSpark Session Stopped. All analysis is complete.")

# Cell 7: Visualization - Content Type Distribution Bar Chart

from pyspark.sql.functions import count, col
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql import SparkSession # Import SparkSession for re-initialization

# Re-initialize Spark Session if it was stopped
if 'spark' not in locals() or spark.sparkContext._jsc is None:
    spark = SparkSession.builder.appName("Netflix_BigData_Analysis").getOrCreate()
    print("Spark Session Re-initialized.")

# Re-load netflix_df as it was created by a previously stopped Spark Session
# Ensure file_path is available or re-declared if needed
file_path = "netflix_titles_nov_2019.csv" # Assuming this file is available
netflix_df = spark.read.csv(
    file_path,
    header=True,
    inferSchema=True,
    multiLine=True,
    escape='"'
)
netflix_df.cache() # Cache for better performance on repeated operations
netflix_df.createOrReplaceTempView("netflix_titles") # Register temp view for SQL

print("--- 1. Counting Content Types (Movies vs. TV Shows) ---")

# Group by 'type' and count the occurrences
content_type_counts_df = netflix_df.groupBy("type").agg(
    count("*").alias("count")
).orderBy(col("count").desc())

# Display the counts
content_type_counts_df.show()

# Convert to Pandas DataFrame for Matplotlib visualization
content_type_counts_pd = content_type_counts_df.toPandas()

# --- 2. Matplotlib Bar Chart Generation ---
plt.figure(figsize=(8, 6))

# Create the bar chart
bars = plt.bar(
    content_type_counts_pd['type'],
    content_type_counts_pd['count'],
    color=['#E50914', '#221F1F'] # Netflix Red and Black
)

# Add titles and labels
plt.title('Distribution of Content Types on Netflix (Movies vs. TV Shows)', fontsize=14)
plt.xlabel('Content Type', fontsize=12)
plt.ylabel('Total Count', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add count labels on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 50, int(yval), ha='center', va='bottom', fontsize=10)

# Save the figure to your Colab environment
output_filename = "content_type_distribution.png"
plt.savefig(output_filename, bbox_inches='tight')
print(f"\nVisualization saved as: {output_filename}")

# Save the underlying data to CSV
content_type_counts_pd.to_csv("content_type_counts.csv", index=False)
print("Data saved as: content_type_counts.csv")

# ==============================================================================
# CELL 1: Full Environment Setup and Stable Spark Initialization
# This setup is optimized for Colab stability to prevent the JAVA_GATEWAY_EXITED error.
# ==============================================================================

# 1. Install necessary libraries
!pip install pyspark findspark plotly pandas matplotlib -q
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# 2. Set Environment Variables
import findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, when, max as max_, udf, avg
from pyspark.sql.window import Window
from pyspark.sql.types import StringType
import plotly.express as px
import matplotlib.pyplot as plt
import pandas as pd

# 3. Stop any existing SparkSession and start a new stable one
if 'spark' in locals():
    spark.stop()

spark = SparkSession.builder\
    .appName("NetflixCompleteAnalysis")\
    .config("spark.driver.memory", "4g")\
    .getOrCreate()

print("Spark Session Initialized Successfully!")

# ==============================================================================
# CELL 2: Netflix Dataset Load and Initial Preparation
# ==============================================================================

file_path = "netflix_titles_nov_2019.csv"

# Load the CSV file into a Spark DataFrame
netflix_df = spark.read.csv(
    file_path,
    header=True,
    inferSchema=True,
    multiLine=True,
    escape='"'
)

# Register as a temp table for SQL in later steps
netflix_df.createOrReplaceTempView("netflix_titles")
netflix_df.cache() # Cache for better performance on repeated operations

print("DataFrame Schema:")
netflix_df.printSchema()
print(f"\nTotal records loaded: {netflix_df.count()}")

# ==============================================================================
# CELL 3: HBase Table Creation (Design) and Insertion Logic
# ==============================================================================

print("--- 1. HBase Table Creation Script (Structural Design) ---")
print("Table Name: netflix_titles")
print("Row Key: show_id")
print("Column Families:")
print("  - info: Stores core attributes (title, release_year, type)")
print("  - metadata: Stores descriptive data (director, country, listed_in)")

# Python class to simulate the insertion object and logic
class HBaseInsertionSimulator:
    def __init__(self, table_name):
        self.table_name = table_name
        self.data_store = {}

    def insert_dataset(self, spark_dataframe):
        """Simulates the Java/Happybase code logic to insert data."""
        print(f"\n--- 2. Python Code Logic to Insert Data (Simulating Happybase) ---")
        rows_to_insert = spark_dataframe.limit(3).collect()
        for row in rows_to_insert:
            row_key = str(row['show_id']).encode('utf-8')
            row_data = {
                b'info:title': str(row['title']).encode('utf-8'),
                b'info:release_year': str(row['release_year']).encode('utf-8'),
                b'metadata:director': str(row['director']).encode('utf-8'),
            }
            self.data_store[row_key] = row_data

# Execute the simulation
hbase_sim = HBaseInsertionSimulator(table_name="netflix_titles")
hbase_sim.insert_dataset(netflix_df)

# ==============================================================================
# CELL 4: MapReduce Queries (RDD - Detailed Genre Count)
# ==============================================================================

print("--- MapReduce: Top 10 Most Frequent Genres ---")

# 1. MAP: Split genres and emit (genre, 1) pairs
genre_rdd = netflix_df.select(col("listed_in")).rdd.filter(
    lambda row: row["listed_in"] is not None
).flatMap(
    # FlatMap splits the comma-separated string and creates (genre, 1) pairs
    lambda row: [(g.strip().lower(), 1) for g in row["listed_in"].split(',')]
)

# 2. REDUCE: Sum the counts for each key (genre)
reduced_rdd = genre_rdd.reduceByKey(lambda a, b: a + b)

# 3. Final step: Convert to DataFrame, sort and save for Plotly
genre_counts_df = reduced_rdd.toDF(["genre", "count"]).orderBy(desc("count"))
genre_counts_df.show(10)

genre_counts_pd = genre_counts_df.toPandas()

# ==============================================================================
# CELL 5: Advanced SQL Queries (Window Functions and UDFs)
# ==============================================================================

# 1. Window Function: Rank titles within each country by release year (Vintage Rank)
print("--- 1. Window Function (Vintage Rank by Country) ---")
ranked_titles = spark.sql("""
    SELECT
        title,
        country,
        release_year,
        -- DENSE_RANK assigns a rank based on release_year within each partition (country)
        DENSE_RANK() OVER (PARTITION BY country ORDER BY release_year ASC) as vintage_rank
    FROM netflix_titles
    WHERE country IS NOT NULL AND release_year IS NOT NULL
""")

ranked_titles.filter(col("country") == "United States").orderBy("vintage_rank").limit(3).show(truncate=False)


# 2. UDF for Text Analysis (Categorizing Description Length)
def categorize_description(text):
    if text is None:
        return "N/A"
    desc_len = len(text)
    if desc_len < 100:
        return "Short"
    elif desc_len < 200:
        return "Medium"
    else:
        return "Long"

# Register the UDF with Spark
spark.udf.register("categorize_description_sql", categorize_description, StringType())

# Use the UDF in a SQL query to analyze description distribution
print("\n--- 2. UDF & SQL Text Analysis (Description Length Distribution) ---")
description_analysis_df = spark.sql("""
    SELECT
        categorize_description_sql(description) AS length_category,
        COUNT(*) AS count
    FROM netflix_titles
    GROUP BY 1
    ORDER BY 2 DESC
""")

description_analysis_df.show(truncate=False)

# ==============================================================================
# CELL 6: Visualizations (Plotly & Matplotlib) and Cleanup
# ==============================================================================

# --- 1. Plotly Visualization (Automated Dashboard Component: Genre Treemap) ---
print("\n--- 1. Automated Dashboard: Plotly Genre Treemap ---")

# Create a Treemap for interactive exploration of genres
fig_genre_treemap = px.treemap(
    genre_counts_pd.head(20), # Use data from MapReduce step
    path=['genre'],
    values='count',
    title='Top 20 Netflix Genre Distribution (Plotly Interactive)',
    color='count',
    color_continuous_scale='Agsunset',
    template='plotly_dark'
)
fig_genre_treemap.show()


# --- 2. Matplotlib Visualization (Content Type Distribution) ---
print("\n--- 2. Matplotlib Visualization: Content Type Distribution ---")

content_type_counts_pd = netflix_df.groupBy("type").agg(count("*").alias("count")).toPandas()

plt.figure(figsize=(8, 6))
bars = plt.bar(
    content_type_counts_pd['type'],
    content_type_counts_pd['count'],
    color=['#E50914', '#221F1F']
)
plt.title('Distribution of Content Types on Netflix (Movies vs. TV Shows)', fontsize=14)
plt.xlabel('Content Type')
plt.ylabel('Total Count')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 50, int(yval), ha='center', va='bottom')

plt.show()

# --- 3. Export Options and Cleanup ---
print("\n--- 3. Export and Cleanup ---")

# Export genre analysis result
genre_counts_pd.to_csv("top_genres_analysis.csv", index=False)
print("Exported: top_genres_analysis.csv")

# Stop Spark session
spark.stop()
print("Spark Session Stopped. All processes complete.")